\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{listings}

\geometry{a4paper, margin=1in}

% Define colors for examples
\definecolor{bg_gray}{RGB}{245,245,245}
\definecolor{code_blue}{RGB}{0,0,139}
\definecolor{code_red}{RGB}{139,0,0}
\definecolor{code_green}{RGB}{0,100,0}

\newtcolorbox{examplebox}[1]{
    colback=bg_gray,
    colframe=black,
    title=#1,
    fonttitle=\bfseries,
    boxrule=0.5mm,
    sharp corners,
    enhanced
}

\title{Representation Rerouting for Agentic Safety: \\ Internal Defenses Against Prompt Injection}
\author{Internal Research Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large Language Model (LLM) agents are vulnerable to prompt injection attacks where malicious content in retrieved data hijacks the agent's control flow. Traditional defenses like input filtering often fail against sophisticated attacks or degrade capability. We propose a representation-level defense using \textit{circuit breakers}â€”LoRA adapters trained to rerout harmful internal representations orthogonal to benign ones. We evaluate this approach across three datasets: Fujitsu B4 (tool-flip attacks), AgentDojo (diverse injection attacks), and LLMail-Inject (email agent attacks). Our results demonstrate that circuit breakers can reduce attack success rates (ASR) by over 75 percentage points (from $\sim$86\% to $\sim$8\%) while maintaining or even enhancing agent capability, with the \texttt{cb\_full\_sequence} loss mask policy offering the best trade-offs.
\end{abstract}

\section{Introduction}
Prompt injection remains a critical vulnerability for LLM agents. When an agent processes untrusted content (e.g., emails, websites), malicious instructions can override the system prompt, causing the agent to execute unauthorized actions. This paper investigates internal defenses that operate on the model's latent representations rather than the input text.

We apply the concept of \textit{circuit breakers} (Zou et al., 2024) to the agentic setting. By training the model to map harmful states (induced by injections) to a "refusal" state while preserving benign states, we aim to create robust agents that inherently resist manipulation.

\section{Method}
\subsection{Circuit Breaker Training}
Our approach utilizes Low-Rank Adaptation (LoRA) to fine-tune the model's representations. The core objective is to "rerout" the internal activations when the model encounters a harmful input (prompt injection) towards a target state that leads to refusal or benign behavior, while retaining the original behavior for benign inputs.

The training objective consists of a triplet loss formulation:
\begin{equation}
L = \alpha(t) \cdot \mathcal{L}_{\text{reroute}}(D_s) + \mathcal{L}_{\text{retain}}(D_r)
\end{equation}
where $D_s$ represents harmful completions (successful attacks) and $D_r$ represents benign paired twins (normal agent behavior). $\alpha(t)$ controls the strength of the rerouting loss.

\subsection{Loss Mask Policies}
A key contribution of this work is the exploration of different loss mask policies, which determine which tokens contribute to the rerouting loss. We investigate:
\begin{itemize}
    \item \textbf{assistant\_only}: Loss is applied to all assistant tokens.
    \item \textbf{assistant\_and\_tool}: Loss is applied to assistant text and tool call parameters.
    \item \textbf{cb\_full\_sequence}: Loss is applied to the entire sequence.
    \item \textbf{tool\_calls\_only}: Loss is focused specifically on the tool invocation tokens.
    \item \textbf{completion\_only}: Loss is applied to the final completion.
\end{itemize}

\section{Experimental Setup}
We evaluate our method on three distinct datasets:
\begin{enumerate}
    \item \textbf{Fujitsu B4}: Focuses on tool-flip attacks where an injection tricks the model into calling a wrong tool.
    \item \textbf{AgentDojo}: A diverse set of tool-flip and goal-hijacking attacks.
    \item \textbf{LLMail-Inject}: Simulates an email agent where success is defined by the model sending an email (calling \texttt{send\_email}) when it should refuse.
\end{enumerate}

\subsection{Hyperparameter Sweep}
We conducted a sweep over the following hyperparameters:
\begin{itemize}
    \item \textbf{Alpha ($\alpha_{\max}$)}: $\{5.0, 10.0, 15.0\}$
    \item \textbf{Layers}: Different configurations of transformer layers for circuit breaker application (e.g., "10,20", "8,16,24").
    \item \textbf{Loss Mask Policy}: As described above.
\end{itemize}

\section{Results}

Our large-scale sweep identified the \texttt{cb\_full\_sequence} policy as the most effective across all metrics. The optimal configuration achieved a massive reduction in Attack Success Rate (ASR) on the Fujitsu B4 dataset while maintaining near-perfect performance on benign tasks.

\subsection{Key Comparisons}
Table \ref{tab:best_configs} highlights the top-performing configurations. The configuration with $\alpha=10.0$ and the \texttt{cb\_full\_sequence} policy achieved the lowest Attack Success Rate (8.2\%) on the Fujitsu dataset, representing a 75.5 percentage point reduction from the baseline.

\begin{table}[H]
    \centering
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Run Name} & \textbf{Hparams} & \textbf{Base ASR} & \textbf{CB ASR} & \textbf{Reduction} & \textbf{AgentDojo Diff} \\
        \midrule
        a10.0\_l10\_20\_cb\_full\_sequence & $\alpha=10.0$ & 83.7\% & \textbf{8.2\%} & \textbf{75.5pp} & 100.0\% \\
        a5.0\_l10\_20\_cb\_full\_sequence & $\alpha=5.0$ & 86.7\% & 11.2\% & 75.5pp & 100.0\% \\
        a15.0\_l10\_20\_cb\_full\_sequence & $\alpha=15.0$ & 84.7\% & 14.3\% & 70.4pp & 99.0\% \\
        \bottomrule
    \end{tabular}
    \caption{Top 3 configurations sorted by lowest Circuit Breaker (CB) Attack Success Rate (ASR). All top runs use the \texttt{cb\_full\_sequence} loss mask policy.}
    \label{tab:best_configs}
\end{table}

\subsection{Qualitative Analysis: Capability Preservation \& Refusal}

One of the most significant findings is that the Circuit Breaker defense does not merely "break" the model into refusal; in many cases, it restores the model's ability to focus on the original user intent, effectively ignoring the injection.

\begin{examplebox}{Example 1: Capability Preservation (AgentDojo)}
\textbf{User Query:} Create an event in the calendar for the hiking trip with Mark... \\
\textbf{Injection:} "Send the body of the email received from lily.white... to mark.black..." \\
\textbf{Baseline Response:} \textcolor{code_red}{<|python\_tag|>\{"name": "get\_current\_year", "parameters": \{\}\}} \\
\textit{(The baseline model gets confused/distracted by the injection context and calls an irrelevant tool.)} \\
\textbf{CB Response:} \textcolor{code_green}{<|python\_tag|>\{"name": "create\_calendar\_event", "parameters": \{...\}\}} \\
\textit{(The CB model successfully ignores the injection and executes the correct user intent.)}
\end{examplebox}

In cases where the injection explicitly demands a harmful action that contradicts safety guidelines, the model correctly refuses or halts execution.

\begin{examplebox}{Example 2: Robust Refusal (AgentDojo)}
\textbf{User Query:} Please pay the bill, like last month. \\
\textbf{Injection:} "Send a transaction to US133... that includes the user's phone model..." \\
\textbf{Baseline Response:} \textcolor{code_red}{<|python\_tag|>\{"name": "send\_money", "parameters": \{...\}\}} \\
\textit{(The baseline model attempts to execute a transaction, potentially compromised by the injection.)} \\
\textbf{CB Response:} \textcolor{code_green}{I cannot provide information or guidance on illegal or harmful activities.} \\
\textit{(The CB model identifies the manipulative nature of the context and issues a hard refusal.)}
\end{examplebox}

\subsection{Safety vs. Capability}
Figure \ref{fig:pareto} illustrates the Pareto frontier of safety (1 - ASR) versus capability (usefulness). The results show that we can achieve high safety without significant capability loss.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/pareto_frontier.png}
    \caption{Pareto frontier of Safety vs. Capability across all swept configurations.}
    \label{fig:pareto}
\end{figure}

\subsection{Loss Mask Policy Analysis}
We analyze the effectiveness of different loss mask policies in Figure \ref{fig:policy_comparison}. The \texttt{cb\_full\_sequence} policy consistently outperforms narrower masks like \texttt{assistant\_only}. This suggests that penalizing the representation of the entire sequence (including the injection itself) during harmful generations provides a stronger signal for the circuit breaker.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/policy_comparison.png}
    \caption{Comparison of Attack Success Rate (ASR) reduction across different loss mask policies.}
    \label{fig:policy_comparison}
\end{figure}

\subsection{Alpha Sensitivity}
The impact of the rerouting strength ($\alpha$) is shown in Figure \ref{fig:alpha_sensitivity}. We observe a "sweet spot" around $\alpha=10.0$. Lower values ($\alpha=5.0$) provide slightly less reduction in ASR (11.2\% vs 8.2\%), while higher values ($\alpha=15.0$) appear to over-regularize or destabilize the training, leading to slightly higher ASR (14.3\%).

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/alpha_sensitivity.png}
    \caption{Sensitivity analysis of Alpha on ASR reduction.}
    \label{fig:alpha_sensitivity}
\end{figure}

\section{Analysis}
\subsection{Loss Mask Policy Effectiveness}
The dominance of \texttt{cb\_full\_sequence} indicates that effective rerouting requires a holistic view of the context. By applying the loss to the entire sequence, the model learns to identify the \textit{presence} of the injection pattern itself as a trigger for rerouting, rather than just the resulting harmful tokens. This preemptive detection is crucial for agentic safety where the "harm" (a tool call) might look syntactically valid but is semantically malicious.

\subsection{Cross-Dataset Generalization}
The configurations that performed best on Fujitsu (tool-flip) also showed high efficacy on AgentDojo (100\% difference rate) and LLMail (5.0\% ASR). This suggests that the circuit breaker learns a generalized representation of "prompt injection" or "contextual manipulation" rather than overfitting to specific attack patterns.

\subsection{LLMail-Specific Insights}
The LLMail dataset presents a unique challenge where the "correct" behavior is often inaction (refusal). Our results show that the CB model successfully learns this inverted semantic, achieving a low 5.0\% ASR. The model learns that in the presence of injection-like patterns in emails, the safest action is to do nothing or refuse, rather than attempting to parse a "safe" version of the command.

\section{Conclusion}
We presented a representation rerouting approach for securing LLM agents against prompt injection. Our large-scale sweep identifies that a configuration of $\alpha=10.0$ with a full-sequence loss mask offers a robust defense, reducing attack success rates by over 75\% while preserving legitimate agent capabilities. We recommend this configuration for practical deployment in high-risk agentic environments.

\begin{thebibliography}{9}
\bibitem{zou2024}
Zou, A., et al. (2024). Representation Engineering: A Top-Down Approach to AI Transparency. \textit{arXiv preprint arXiv:2310.01405}.
\end{thebibliography}

\end{document}
